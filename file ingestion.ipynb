{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51e7f7b4-0515-4f70-a835-b8edc19fd683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2GB dataset generated and saved as 'large_employee_records.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of rows and columns\n",
    "num_rows = 10_000_000  # 10 million rows\n",
    "num_columns = 10       # 10 columns\n",
    "\n",
    "# Helper function to generate random strings\n",
    "def random_string(length=10):\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for _ in range(length))\n",
    "\n",
    "# Create random data for each column\n",
    "data = {\n",
    "    \"id\": np.arange(1, num_rows + 1),                             # Unique ID\n",
    "    \"age\": np.random.randint(18, 80, size=num_rows),              # Random age between 18 and 80\n",
    "    \"salary\": np.random.uniform(30000, 120000, size=num_rows),     # Random float for salary\n",
    "    \"date_of_joining\": pd.date_range('2000-01-01', periods=num_rows, freq='T'),  # Random dates\n",
    "    \"department\": np.random.choice(['HR', 'Finance', 'IT', 'Sales'], size=num_rows),  # Random department\n",
    "    \"email\": [random_string() + \"@example.com\" for _ in range(num_rows)],  # Random email strings\n",
    "    \"city\": np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston'], size=num_rows),  # Cities\n",
    "    \"performance_score\": np.random.randint(1, 10, size=num_rows),  # Performance score (1-10)\n",
    "    \"gender\": np.random.choice(['Male', 'Female'], size=num_rows),  # Random gender\n",
    "    \"phone_number\": ['+1-' + ''.join(np.random.choice(list('0123456789'), 10)) for _ in range(num_rows)],  # Random phone numbers\n",
    "}\n",
    "\n",
    "# Convert the dictionary into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV (without index)\n",
    "df.to_csv(\"large_employee_records.csv\", index=False)\n",
    "\n",
    "print(\"2GB dataset generated and saved as 'large_employee_records.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10e957f9-cb47-4577-9b0b-ccc72a6e264f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  age         salary      date_of_joining department  \\\n",
      "0   1   56  105443.188596  2000-01-01 00:00:00         IT   \n",
      "1   2   69   87745.794238  2000-01-01 00:01:00         HR   \n",
      "2   3   46  108896.424998  2000-01-01 00:02:00         HR   \n",
      "3   4   32  119826.816879  2000-01-01 00:03:00    Finance   \n",
      "4   5   60   91009.982737  2000-01-01 00:04:00         HR   \n",
      "\n",
      "                    email         city  performance_score  gender  \\\n",
      "0  ydzgrybrmk@example.com      Chicago                  6  Female   \n",
      "1  qdmlctycoq@example.com      Chicago                  5  Female   \n",
      "2  uukgzlztpf@example.com  Los Angeles                  4  Female   \n",
      "3  fhefgoqeca@example.com     New York                  5    Male   \n",
      "4  cpdpyarzks@example.com  Los Angeles                  7  Female   \n",
      "\n",
      "    phone_number  \n",
      "0  +1-7228155984  \n",
      "1  +1-2029290494  \n",
      "2  +1-9200789379  \n",
      "3  +1-8350371798  \n",
      "4  +1-6897024588  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file in chunks to handle large size\n",
    "chunk_size = 1_000_000\n",
    "chunks = pd.read_csv(\"large_employee_records.csv\", chunksize=chunk_size)\n",
    "\n",
    "# Example: Process and combine chunks\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd14f586-1ca8-4d97-95a2-cf13cb01622c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  age         salary      date_of_joining department  \\\n",
      "0   1   56  105443.188596  2000-01-01 00:00:00         IT   \n",
      "1   2   69   87745.794238  2000-01-01 00:01:00         HR   \n",
      "2   3   46  108896.424998  2000-01-01 00:02:00         HR   \n",
      "3   4   32  119826.816879  2000-01-01 00:03:00    Finance   \n",
      "4   5   60   91009.982737  2000-01-01 00:04:00         HR   \n",
      "\n",
      "                    email         city  performance_score  gender  \\\n",
      "0  ydzgrybrmk@example.com      Chicago                  6  Female   \n",
      "1  qdmlctycoq@example.com      Chicago                  5  Female   \n",
      "2  uukgzlztpf@example.com  Los Angeles                  4  Female   \n",
      "3  fhefgoqeca@example.com     New York                  5    Male   \n",
      "4  cpdpyarzks@example.com  Los Angeles                  7  Female   \n",
      "\n",
      "    phone_number  \n",
      "0  +1-7228155984  \n",
      "1  +1-2029290494  \n",
      "2  +1-9200789379  \n",
      "3  +1-8350371798  \n",
      "4  +1-6897024588  \n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read the CSV file with Dask\n",
    "df_dask = dd.read_csv(\"large_employee_records.csv\")\n",
    "\n",
    "# Compute to load into memory (optional, based on your needs)\n",
    "df = df_dask.compute()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6034e164-8c53-420e-801c-7e7987c830e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names cleaned and dataset saved as 'large_employee_records_cleaned.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Clean column names\n",
    "df.columns = [col.strip().replace(' ', '_').lower() for col in df.columns]\n",
    "\n",
    "# Optionally save the cleaned dataset\n",
    "df.to_csv(\"large_employee_records_cleaned.csv\", index=False)\n",
    "print(\"Column names cleaned and dataset saved as 'large_employee_records_cleaned.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3d52094-61c9-4048-8a40-7aa67758f322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as 'large_employee_records.txt.gz' in pipe-separated format.\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"large_employee_records.txt.gz\", sep='|', compression='gzip', index=False)\n",
    "print(\"File saved as 'large_employee_records.txt.gz' in pipe-separated format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bbc2d23-1f0f-45ef-85a9-f88f0c39e9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 10000000\n",
      "Total number of columns: 10\n",
      "File size: 345.98 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Read the pipe-separated text file\n",
    "df_summary = pd.read_csv(\"large_employee_records.txt.gz\", sep='|')\n",
    "\n",
    "# Generate summary\n",
    "total_rows = len(df_summary)\n",
    "total_columns = len(df_summary.columns)\n",
    "file_size = os.path.getsize(\"large_employee_records.txt.gz\") / (1024 * 1024)  # Size in GB\n",
    "\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "print(f\"Total number of columns: {total_columns}\")\n",
    "print(f\"File size: {file_size:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d57b813-a14b-49d6-9fce-bd4269955777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML schema created and saved as 'schema.yaml'.\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Define expected columns based on your dataset\n",
    "expected_columns = df.columns.tolist()\n",
    "\n",
    "# Create a schema dictionary\n",
    "schema = {'columns': expected_columns}\n",
    "\n",
    "# Write schema to a YAML file\n",
    "with open('schema.yaml', 'w') as file:\n",
    "    yaml.dump(schema, file, default_flow_style=False)\n",
    "\n",
    "print(\"YAML schema created and saved as 'schema.yaml'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5111b522-f626-40a7-a0f6-b9c90543746e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names match the schema.\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Load the schema from the YAML file\n",
    "with open('schema.yaml', 'r') as file:\n",
    "    schema = yaml.safe_load(file)\n",
    "\n",
    "expected_columns = schema['columns']\n",
    "\n",
    "# Validate that the actual dataset columns match the expected schema\n",
    "actual_columns = df.columns.tolist()\n",
    "\n",
    "if set(expected_columns) == set(actual_columns):\n",
    "    print(\"Column names match the schema.\")\n",
    "else:\n",
    "    print(\"Column names do not match the schema.\")\n",
    "    print(f\"Expected columns: {expected_columns}\")\n",
    "    print(f\"Actual columns: {actual_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "051d4dc0-4217-4433-8c3f-7ef3b3d37b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the dataset accurately match the YAML schema.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"large_employee_records_cleaned.csv\")\n",
    "\n",
    "# Load the YAML schema file\n",
    "with open('schema.yaml', 'r') as file:\n",
    "    schema = yaml.safe_load(file)\n",
    "\n",
    "# Extract the expected column names from the YAML schema\n",
    "expected_columns = schema['columns']\n",
    "\n",
    "# Extract the actual column names from the dataset\n",
    "actual_columns = df.columns.tolist()\n",
    "\n",
    "# Check if the actual columns match the expected columns\n",
    "if set(expected_columns) == set(actual_columns):\n",
    "    print(\"Column names in the dataset accurately match the YAML schema.\")\n",
    "else:\n",
    "    print(\"Mismatch in columns!\")\n",
    "    print(f\"Expected columns: {expected_columns}\")\n",
    "    print(f\"Actual columns: {actual_columns}\")\n",
    "    \n",
    "    # Find missing columns\n",
    "    missing_in_data = set(expected_columns) - set(actual_columns)\n",
    "    if missing_in_data:\n",
    "        print(f\"Columns in YAML but missing in dataset: {missing_in_data}\")\n",
    "    \n",
    "    # Find extra columns\n",
    "    extra_in_data = set(actual_columns) - set(expected_columns)\n",
    "    if extra_in_data:\n",
    "        print(f\"Extra columns in dataset not in YAML: {extra_in_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f20d5b3-2b59-4d4b-913c-af6920df31ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Column names in the dataset match the schema exactly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"large_employee_records_cleaned.csv\")\n",
    "\n",
    "# Load the YAML schema file\n",
    "with open('schema.yaml', 'r') as file:\n",
    "    schema = yaml.safe_load(file)\n",
    "\n",
    "# Extract the expected column names from the YAML schema\n",
    "expected_columns = schema['columns']\n",
    "\n",
    "# Extract the actual column names from the dataset\n",
    "actual_columns = df.columns.tolist()\n",
    "\n",
    "# Validate column names\n",
    "if expected_columns == actual_columns:\n",
    "    print(\"Success: Column names in the dataset match the schema exactly.\")\n",
    "else:\n",
    "    print(\"Error: Column names do not match the schema.\")\n",
    "    \n",
    "    # Find missing or extra columns\n",
    "    missing_columns = set(expected_columns) - set(actual_columns)\n",
    "    extra_columns = set(actual_columns) - set(expected_columns)\n",
    "\n",
    "    if missing_columns:\n",
    "        print(f\"Missing columns in dataset: {missing_columns}\")\n",
    "    \n",
    "    if extra_columns:\n",
    "        print(f\"Extra columns in dataset: {extra_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3dc76cb8-80b0-480d-8c79-f6a1f1ff59bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  age         salary      date_of_joining department  \\\n",
      "0   1   56  105443.188596  2000-01-01 00:00:00         IT   \n",
      "1   2   69   87745.794238  2000-01-01 00:01:00         HR   \n",
      "2   3   46  108896.424998  2000-01-01 00:02:00         HR   \n",
      "3   4   32  119826.816879  2000-01-01 00:03:00    Finance   \n",
      "4   5   60   91009.982737  2000-01-01 00:04:00         HR   \n",
      "\n",
      "                    email         city  performance_score  gender  \\\n",
      "0  ydzgrybrmk@example.com      Chicago                  6  Female   \n",
      "1  qdmlctycoq@example.com      Chicago                  5  Female   \n",
      "2  uukgzlztpf@example.com  Los Angeles                  4  Female   \n",
      "3  fhefgoqeca@example.com     New York                  5    Male   \n",
      "4  cpdpyarzks@example.com  Los Angeles                  7  Female   \n",
      "\n",
      "    phone_number  \n",
      "0  +1-7228155984  \n",
      "1  +1-2029290494  \n",
      "2  +1-9200789379  \n",
      "3  +1-8350371798  \n",
      "4  +1-6897024588  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
