{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51e7f7b4-0515-4f70-a835-b8edc19fd683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2GB dataset generated and saved as 'large_employee_records.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of rows and columns\n",
    "num_rows = 10_000_000  # 10 million rows\n",
    "num_columns = 10       # 10 columns\n",
    "\n",
    "# Helper function to generate random strings\n",
    "def random_string(length=10):\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for _ in range(length))\n",
    "\n",
    "# Create random data for each column\n",
    "data = {\n",
    "    \"id\": np.arange(1, num_rows + 1),                             # Unique ID\n",
    "    \"age\": np.random.randint(18, 80, size=num_rows),              # Random age between 18 and 80\n",
    "    \"salary\": np.random.uniform(30000, 120000, size=num_rows),     # Random float for salary\n",
    "    \"date_of_joining\": pd.date_range('2000-01-01', periods=num_rows, freq='T'),  # Random dates\n",
    "    \"department\": np.random.choice(['HR', 'Finance', 'IT', 'Sales'], size=num_rows),  # Random department\n",
    "    \"email\": [random_string() + \"@example.com\" for _ in range(num_rows)],  # Random email strings\n",
    "    \"city\": np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston'], size=num_rows),  # Cities\n",
    "    \"performance_score\": np.random.randint(1, 10, size=num_rows),  # Performance score (1-10)\n",
    "    \"gender\": np.random.choice(['Male', 'Female'], size=num_rows),  # Random gender\n",
    "    \"phone_number\": ['+1-' + ''.join(np.random.choice(list('0123456789'), 10)) for _ in range(num_rows)],  # Random phone numbers\n",
    "}\n",
    "\n",
    "# Convert the dictionary into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV (without index)\n",
    "df.to_csv(\"large_employee_records.csv\", index=False)\n",
    "\n",
    "print(\"2GB dataset generated and saved as 'large_employee_records.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10e957f9-cb47-4577-9b0b-ccc72a6e264f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  age         salary      date_of_joining department  \\\n",
      "0   1   56  105443.188596  2000-01-01 00:00:00         IT   \n",
      "1   2   69   87745.794238  2000-01-01 00:01:00         HR   \n",
      "2   3   46  108896.424998  2000-01-01 00:02:00         HR   \n",
      "3   4   32  119826.816879  2000-01-01 00:03:00    Finance   \n",
      "4   5   60   91009.982737  2000-01-01 00:04:00         HR   \n",
      "\n",
      "                    email         city  performance_score  gender  \\\n",
      "0  ydzgrybrmk@example.com      Chicago                  6  Female   \n",
      "1  qdmlctycoq@example.com      Chicago                  5  Female   \n",
      "2  uukgzlztpf@example.com  Los Angeles                  4  Female   \n",
      "3  fhefgoqeca@example.com     New York                  5    Male   \n",
      "4  cpdpyarzks@example.com  Los Angeles                  7  Female   \n",
      "\n",
      "    phone_number  \n",
      "0  +1-7228155984  \n",
      "1  +1-2029290494  \n",
      "2  +1-9200789379  \n",
      "3  +1-8350371798  \n",
      "4  +1-6897024588  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file in chunks to handle large size\n",
    "chunk_size = 1_000_000\n",
    "chunks = pd.read_csv(\"large_employee_records.csv\", chunksize=chunk_size)\n",
    "\n",
    "# Example: Process and combine chunks\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef6bc3b2-e759-474a-b43c-f017e04ea98f",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Read the CSV file in chunks (optional for large files) or in one go\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m df_pandas \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlarge_employee_records.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# End the timer\u001b[39;00m\n\u001b[0;32m     11\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1724\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:579\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:668\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2050\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the CSV file in chunks (optional for large files) or in one go\n",
    "df_pandas = pd.read_csv(\"large_employee_records.csv\")\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the total time taken\n",
    "pandas_time = end_time - start_time\n",
    "print(f\"Pandas took {pandas_time:.2f} seconds to read the file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd14f586-1ca8-4d97-95a2-cf13cb01622c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  age         salary      date_of_joining department  \\\n",
      "0   1   56  105443.188596  2000-01-01 00:00:00         IT   \n",
      "1   2   69   87745.794238  2000-01-01 00:01:00         HR   \n",
      "2   3   46  108896.424998  2000-01-01 00:02:00         HR   \n",
      "3   4   32  119826.816879  2000-01-01 00:03:00    Finance   \n",
      "4   5   60   91009.982737  2000-01-01 00:04:00         HR   \n",
      "\n",
      "                    email         city  performance_score  gender  \\\n",
      "0  ydzgrybrmk@example.com      Chicago                  6  Female   \n",
      "1  qdmlctycoq@example.com      Chicago                  5  Female   \n",
      "2  uukgzlztpf@example.com  Los Angeles                  4  Female   \n",
      "3  fhefgoqeca@example.com     New York                  5    Male   \n",
      "4  cpdpyarzks@example.com  Los Angeles                  7  Female   \n",
      "\n",
      "    phone_number  \n",
      "0  +1-7228155984  \n",
      "1  +1-2029290494  \n",
      "2  +1-9200789379  \n",
      "3  +1-8350371798  \n",
      "4  +1-6897024588  \n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read the CSV file with Dask\n",
    "df_dask = dd.read_csv(\"large_employee_records.csv\")\n",
    "\n",
    "# Compute to load into memory (optional, based on your needs)\n",
    "df = df_dask.compute()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ccb25b4-b7c9-4487-b47f-8330ef6b3f48",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 13] Permission denied",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dask\\backends.py:136\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:763\u001b[0m, in \u001b[0;36mmake_reader.<locals>.read\u001b[1;34m(urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\n\u001b[0;32m    751\u001b[0m     urlpath,\n\u001b[0;32m    752\u001b[0m     blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    762\u001b[0m ):\n\u001b[1;32m--> 763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m read_pandas(\n\u001b[0;32m    764\u001b[0m         reader,\n\u001b[0;32m    765\u001b[0m         urlpath,\n\u001b[0;32m    766\u001b[0m         blocksize\u001b[38;5;241m=\u001b[39mblocksize,\n\u001b[0;32m    767\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m    768\u001b[0m         compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    769\u001b[0m         sample\u001b[38;5;241m=\u001b[39msample,\n\u001b[0;32m    770\u001b[0m         sample_rows\u001b[38;5;241m=\u001b[39msample_rows,\n\u001b[0;32m    771\u001b[0m         enforce\u001b[38;5;241m=\u001b[39menforce,\n\u001b[0;32m    772\u001b[0m         assume_missing\u001b[38;5;241m=\u001b[39massume_missing,\n\u001b[0;32m    773\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    774\u001b[0m         include_path_column\u001b[38;5;241m=\u001b[39minclude_path_column,\n\u001b[0;32m    775\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    776\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:563\u001b[0m, in \u001b[0;36mread_pandas\u001b[1;34m(reader, urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m b_lineterminator \u001b[38;5;241m=\u001b[39m lineterminator\u001b[38;5;241m.\u001b[39mencode()\n\u001b[1;32m--> 563\u001b[0m b_out \u001b[38;5;241m=\u001b[39m read_bytes(\n\u001b[0;32m    564\u001b[0m     urlpath,\n\u001b[0;32m    565\u001b[0m     delimiter\u001b[38;5;241m=\u001b[39mb_lineterminator,\n\u001b[0;32m    566\u001b[0m     blocksize\u001b[38;5;241m=\u001b[39mblocksize,\n\u001b[0;32m    567\u001b[0m     sample\u001b[38;5;241m=\u001b[39msample,\n\u001b[0;32m    568\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    569\u001b[0m     include_path\u001b[38;5;241m=\u001b[39minclude_path_column,\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[0;32m    571\u001b[0m )\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_path_column:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dask\\bytes\\core.py:173\u001b[0m, in \u001b[0;36mread_bytes\u001b[1;34m(urlpath, delimiter, not_zero, blocksize, sample, compression, include_path, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 173\u001b[0m     sample_buff \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread(sample)\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\fsspec\\implementations\\local.py:366\u001b[0m, in \u001b[0;36mLocalFileOpener.read\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Read the CSV file using Dask\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m df_dask \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlarge_employee_records.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Trigger computation (necessary in Dask)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m df_dask \u001b[38;5;241m=\u001b[39m df_dask\u001b[38;5;241m.\u001b[39mcompute()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dask\\backends.py:138\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname(func)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod registered to the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m backend.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mPermissionError\u001b[0m: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 13] Permission denied"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the CSV file using Dask\n",
    "df_dask = dd.read_csv(\"large_employee_records.csv\")\n",
    "\n",
    "# Trigger computation (necessary in Dask)\n",
    "df_dask = df_dask.compute()\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the total time taken\n",
    "dask_time = end_time - start_time\n",
    "print(f\"Dask took {dask_time:.2f} seconds to read the file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6034e164-8c53-420e-801c-7e7987c830e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names cleaned and dataset saved as 'large_employee_records_cleaned.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Clean column names\n",
    "df.columns = [col.strip().replace(' ', '_').lower() for col in df.columns]\n",
    "\n",
    "# Optionally save the cleaned dataset\n",
    "df.to_csv(\"large_employee_records_cleaned.csv\", index=False)\n",
    "print(\"Column names cleaned and dataset saved as 'large_employee_records_cleaned.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3d52094-61c9-4048-8a40-7aa67758f322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as 'large_employee_records.txt.gz' in pipe-separated format.\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"large_employee_records.txt.gz\", sep='|', compression='gzip', index=False)\n",
    "print(\"File saved as 'large_employee_records.txt.gz' in pipe-separated format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bbc2d23-1f0f-45ef-85a9-f88f0c39e9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 10000000\n",
      "Total number of columns: 10\n",
      "File size: 345.98 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Read the pipe-separated text file\n",
    "df_summary = pd.read_csv(\"large_employee_records.txt.gz\", sep='|')\n",
    "\n",
    "# Generate summary\n",
    "total_rows = len(df_summary)\n",
    "total_columns = len(df_summary.columns)\n",
    "file_size = os.path.getsize(\"large_employee_records.txt.gz\") / (1024 * 1024)  # Size in GB\n",
    "\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "print(f\"Total number of columns: {total_columns}\")\n",
    "print(f\"File size: {file_size:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d57b813-a14b-49d6-9fce-bd4269955777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML schema created and saved as 'schema.yaml'.\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Define expected columns based on your dataset\n",
    "expected_columns = df.columns.tolist()\n",
    "\n",
    "# Create a schema dictionary\n",
    "schema = {'columns': expected_columns}\n",
    "\n",
    "# Write schema to a YAML file\n",
    "with open('schema.yaml', 'w') as file:\n",
    "    yaml.dump(schema, file, default_flow_style=False)\n",
    "\n",
    "print(\"YAML schema created and saved as 'schema.yaml'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5111b522-f626-40a7-a0f6-b9c90543746e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names match the schema.\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Load the schema from the YAML file\n",
    "with open('schema.yaml', 'r') as file:\n",
    "    schema = yaml.safe_load(file)\n",
    "\n",
    "expected_columns = schema['columns']\n",
    "\n",
    "# Validate that the actual dataset columns match the expected schema\n",
    "actual_columns = df.columns.tolist()\n",
    "\n",
    "if set(expected_columns) == set(actual_columns):\n",
    "    print(\"Column names match the schema.\")\n",
    "else:\n",
    "    print(\"Column names do not match the schema.\")\n",
    "    print(f\"Expected columns: {expected_columns}\")\n",
    "    print(f\"Actual columns: {actual_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "051d4dc0-4217-4433-8c3f-7ef3b3d37b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the dataset accurately match the YAML schema.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"large_employee_records_cleaned.csv\")\n",
    "\n",
    "# Load the YAML schema file\n",
    "with open('schema.yaml', 'r') as file:\n",
    "    schema = yaml.safe_load(file)\n",
    "\n",
    "# Extract the expected column names from the YAML schema\n",
    "expected_columns = schema['columns']\n",
    "\n",
    "# Extract the actual column names from the dataset\n",
    "actual_columns = df.columns.tolist()\n",
    "\n",
    "# Check if the actual columns match the expected columns\n",
    "if set(expected_columns) == set(actual_columns):\n",
    "    print(\"Column names in the dataset accurately match the YAML schema.\")\n",
    "else:\n",
    "    print(\"Mismatch in columns!\")\n",
    "    print(f\"Expected columns: {expected_columns}\")\n",
    "    print(f\"Actual columns: {actual_columns}\")\n",
    "    \n",
    "    # Find missing columns\n",
    "    missing_in_data = set(expected_columns) - set(actual_columns)\n",
    "    if missing_in_data:\n",
    "        print(f\"Columns in YAML but missing in dataset: {missing_in_data}\")\n",
    "    \n",
    "    # Find extra columns\n",
    "    extra_in_data = set(actual_columns) - set(expected_columns)\n",
    "    if extra_in_data:\n",
    "        print(f\"Extra columns in dataset not in YAML: {extra_in_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f20d5b3-2b59-4d4b-913c-af6920df31ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Column names in the dataset match the schema exactly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"large_employee_records_cleaned.csv\")\n",
    "\n",
    "# Load the YAML schema file\n",
    "with open('schema.yaml', 'r') as file:\n",
    "    schema = yaml.safe_load(file)\n",
    "\n",
    "# Extract the expected column names from the YAML schema\n",
    "expected_columns = schema['columns']\n",
    "\n",
    "# Extract the actual column names from the dataset\n",
    "actual_columns = df.columns.tolist()\n",
    "\n",
    "# Validate column names\n",
    "if expected_columns == actual_columns:\n",
    "    print(\"Success: Column names in the dataset match the schema exactly.\")\n",
    "else:\n",
    "    print(\"Error: Column names do not match the schema.\")\n",
    "    \n",
    "    # Find missing or extra columns\n",
    "    missing_columns = set(expected_columns) - set(actual_columns)\n",
    "    extra_columns = set(actual_columns) - set(expected_columns)\n",
    "\n",
    "    if missing_columns:\n",
    "        print(f\"Missing columns in dataset: {missing_columns}\")\n",
    "    \n",
    "    if extra_columns:\n",
    "        print(f\"Extra columns in dataset: {extra_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3dc76cb8-80b0-480d-8c79-f6a1f1ff59bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  age         salary      date_of_joining department  \\\n",
      "0   1   56  105443.188596  2000-01-01 00:00:00         IT   \n",
      "1   2   69   87745.794238  2000-01-01 00:01:00         HR   \n",
      "2   3   46  108896.424998  2000-01-01 00:02:00         HR   \n",
      "3   4   32  119826.816879  2000-01-01 00:03:00    Finance   \n",
      "4   5   60   91009.982737  2000-01-01 00:04:00         HR   \n",
      "\n",
      "                    email         city  performance_score  gender  \\\n",
      "0  ydzgrybrmk@example.com      Chicago                  6  Female   \n",
      "1  qdmlctycoq@example.com      Chicago                  5  Female   \n",
      "2  uukgzlztpf@example.com  Los Angeles                  4  Female   \n",
      "3  fhefgoqeca@example.com     New York                  5    Male   \n",
      "4  cpdpyarzks@example.com  Los Angeles                  7  Female   \n",
      "\n",
      "    phone_number  \n",
      "0  +1-7228155984  \n",
      "1  +1-2029290494  \n",
      "2  +1-9200789379  \n",
      "3  +1-8350371798  \n",
      "4  +1-6897024588  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d03ca23d-313d-4573-9e53-e86dc6e9f925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas took 53.63 seconds to read the compressed file.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the compressed CSV file\n",
    "df_pandas = pd.read_csv(\"large_employee_records.txt.gz\", compression='gzip')\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the total time taken\n",
    "pandas_time = end_time - start_time\n",
    "print(f\"Pandas took {pandas_time:.2f} seconds to read the compressed file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ebd882b3-64e1-42f6-aa0e-6134fcd43e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Warning gzip compression does not support breaking apart files\n",
      "Please ensure that each individual file can fit in memory and\n",
      "use the keyword ``blocksize=None to remove this message``\n",
      "Setting ``blocksize=None``\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask took 76.38 seconds to read the compressed file.\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the compressed CSV file using Dask\n",
    "df_dask = dd.read_csv(\"large_employee_records.txt.gz\", compression='gzip')\n",
    "\n",
    "# Trigger computation\n",
    "df_dask = df_dask.compute()\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the total time taken\n",
    "dask_time = end_time - start_time\n",
    "print(f\"Dask took {dask_time:.2f} seconds to read the compressed file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3fd603-37e0-4710-a2be-dd1a8eb4cbdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
